---
title: LangGraph
description: "Adds instrumentation for the LangGraph SDK."
supported:
  - javascript.node
  - javascript.aws-lambda
  - javascript.azure-functions
  - javascript.connect
  - javascript.express
  - javascript.fastify
  - javascript.gcp-functions
  - javascript.hapi
  - javascript.hono
  - javascript.koa
  - javascript.nestjs
  - javascript.electron
  - javascript.nextjs
  - javascript.nuxt
  - javascript.solidstart
  - javascript.sveltekit
  - javascript.react-router
  - javascript.remix
  - javascript.astro
  - javascript.bun
  - javascript.tanstackstart-react
  - javascript.cloudflare
  - javascript
  - javascript.react
  - javascript.angular
  - javascript.vue
  - javascript.svelte
  - javascript.solid
  - javascript.ember
  - javascript.gatsby
---

<PlatformSection notSupported={["javascript", "javascript.react", "javascript.angular", "javascript.vue", "javascript.svelte", "javascript.solid", "javascript.ember", "javascript.gatsby"]}>

## Server-Side Usage

_Import name: `Sentry.langGraphIntegration`_

The `langGraphIntegration` adds instrumentation for [`@langchain/langgraph`](https://www.npmjs.com/package/@langchain/langgraph) to capture spans by automatically wrapping LangGraph operations and recording AI agent interactions including agent invocations, graph executions, and node operations. 


<PlatformSection notSupported={["javascript.nextjs", "javascript.nuxt", "javascript.solidstart", "javascript.sveltekit", "javascript.react-router", "javascript.remix", "javascript.astro", "javascript.tanstackstart-react", "javascript.bun", "javascript.cloudflare"]}>

<Alert>

Enabled by default and automatically captures spans for LangGraph SDK calls. Requires Sentry SDK version `10.28.0` or higher.

</Alert>

</PlatformSection>

<PlatformSection supported={["javascript.nextjs", "javascript.nuxt", "javascript.solidstart", "javascript.sveltekit", "javascript.react-router", "javascript.remix", "javascript.astro", "javascript.tanstackstart-react", "javascript.bun", "javascript.cloudflare"]}>

<Alert>

Enabled by default and automatically captures spans for LangGraph SDK calls. Requires Sentry SDK version `10.28.0` or higher.

For other runtimes, like the Browser, the instrumentation needs to be manually enabled. See the [setup instructions below](#browser-side-usage).

</Alert>

</PlatformSection>

To customize what data is captured (such as inputs and outputs), see the [Options](#options) in the Configuration section.

</PlatformSection>

<PlatformSection supported={["javascript", "javascript.react", "javascript.angular", "javascript.vue", "javascript.svelte", "javascript.solid", "javascript.ember", "javascript.gatsby", "javascript.nextjs", "javascript.nuxt", "javascript.solidstart", "javascript.sveltekit", "javascript.react-router", "javascript.remix", "javascript.astro", "javascript.tanstackstart-react", "javascript.electron", "javascript.cloudflare"]}>

## Browser-Side Usage

_Import name: `Sentry.instrumentLangGraph`_

<PlatformSection supported={["javascript.cloudflare"]}>

<Alert>

For Cloudflare Workers, manual instrumentation is required using `instrumentLangGraph`.

</Alert>

</PlatformSection>

The `instrumentLangGraph` helper adds instrumentation for [`@langchain/langgraph`](https://www.npmjs.com/package/@langchain/langgraph) to capture spans by wrapping a compiled LangGraph graph and recording AI agent interactions with configurable input/output recording. You need to manually wrap your compiled graph with this helper. See example below:

```javascript
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation, START, END } from '@langchain/langgraph';

// Create LLM call
const llm = new ChatOpenAI({
  modelName: "gpt-4o",
  apiKey: "your-api-key", // Warning: API key will be exposed in browser!
});

async function callLLM(state) {
  const response = await llm.invoke(state.messages);

  return {
    messages: [...state.messages, response],
  };
}

// Create the agent
const agent = new StateGraph(MessagesAnnotation)
  .addNode('agent', callLLM)
  .addEdge(START, 'agent')
  .addEdge('agent', END);

const graph = agent.compile({ name: 'my_agent' });

// Manually instrument the graph
Sentry.instrumentLangGraph(graph, {
  recordInputs: true,
  recordOutputs: true,
});

// Invoke the agent
const result = await graph.invoke({
  messages: [
    new SystemMessage("You are a helpful assistant."),
    new HumanMessage("Hello!"),
  ],
});
```

To customize what data is captured (such as inputs and outputs), see the [Options](#options) in the Configuration section.

</PlatformSection>

<PlatformSection supported={['javascript.nextjs']}>

## Edge runtime

This integration is automatically instrumented in the Node.js runtime. For Next.js applications using the Edge runtime, you need to manually instrument LangGraph operations using the `instrumentLangGraph` helper:

```javascript
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation, START, END } from '@langchain/langgraph';

// Create LLM call
const llm = new ChatOpenAI({
  modelName: "gpt-4o",
  apiKey: "your-api-key", // Warning: API key will be exposed in browser!
});

async function callLLM(state) {
  const response = await llm.invoke(state.messages);

  return {
    messages: [...state.messages, response],
  };
}

// Create the agent
const agent = new StateGraph(MessagesAnnotation)
  .addNode('agent', callLLM)
  .addEdge(START, 'agent')
  .addEdge('agent', END);

const graph = agent.compile({ name: 'my_agent' });

// Manually instrument the graph
Sentry.instrumentLangGraph(graph, {
  recordInputs: true,
  recordOutputs: true,
});

// Invoke the agent
const result = await graph.invoke({
  messages: [
    new SystemMessage("You are a helpful assistant."),
    new HumanMessage("Hello!"),
  ],
});
```

</PlatformSection>

## Configuration

### Options

The following options control what data is captured from LangGraph operations:

#### `recordInputs`

_Type: `boolean` (optional)_

Records inputs to LangGraph operations (such as messages and state data passed to the graph).

Defaults to `true` if `sendDefaultPii` is `true`.

#### `recordOutputs`

_Type: `boolean` (optional)_

Records outputs from LangGraph operations (such as generated responses, agent outputs, and final state).

Defaults to `true` if `sendDefaultPii` is `true`.

**Usage**

<PlatformSection notSupported={["javascript", "javascript.react", "javascript.angular", "javascript.vue", "javascript.svelte", "javascript.solid", "javascript.ember", "javascript.gatsby"]}>

Using the `langGraphIntegration` integration:

```javascript
Sentry.init({
  dsn: "____PUBLIC_DSN____",
  // Tracing must be enabled for agent monitoring to work
  tracesSampleRate: 1.0,
  integrations: [
    Sentry.langGraphIntegration({
      // your options here
    }),
  ],
});
```

</PlatformSection>

<PlatformSection supported={["javascript", "javascript.react", "javascript.angular", "javascript.vue", "javascript.svelte", "javascript.solid", "javascript.ember", "javascript.gatsby", "javascript.nextjs", "javascript.nuxt", "javascript.solidstart", "javascript.sveltekit", "javascript.react-router", "javascript.remix", "javascript.astro", "javascript.tanstackstart-react", "javascript.electron", "javascript.cloudflare"]}>

Using the `instrumentLangGraph` helper:

```javascript
Sentry.instrumentLangGraph(graph, {
  // your options here
});
```

</PlatformSection>

## Supported Operations

By default, tracing support is added to the following LangGraph SDK calls:

- **Agent Creation** (`gen_ai.create_agent`) - Captures spans when compiling a StateGraph into an executable agent
- **Agent Invocation** (`gen_ai.invoke_agent`) - Captures spans for agent execution via `invoke()`


## Supported Versions

- `@langchain/langgraph`: `>=0.2.0 <2.0.0`
