---
title: LiteLLM
description: "Adds instrumentation for the LiteLLM Python SDK."
---

This integration connects Sentry with the [LiteLLM Python SDK](https://github.com/BerriAI/litellm) to automatically capture spans for LLM requests. Data shows up in the [AI Agents Dashboard](/product/insights/ai/agents).

## Install

Install `sentry-sdk` from PyPI:

```bash {tabTitle:pip}
pip install sentry-sdk
```

```bash {tabTitle:uv}
uv add sentry-sdk
```

## Configure

Add `LiteLLMIntegration()` to your `integrations` list:

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    # Set traces_sample_rate to 1.0 to capture 100%
    # of transactions for tracing.
    traces_sample_rate=1.0,
    # Add data like inputs and responses;
    # see https://docs.sentry.io/platforms/python/data-management/data-collected/ for more info
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(),
    ],
)
```

## Verify

Verify that the integration works by making a chat completion request to LiteLLM.

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration
import litellm

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    traces_sample_rate=1.0,
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(),
    ],
)

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "say hello"}],
    max_tokens=100
)
print(response.choices[0].message.content)
```

After running this script, the resulting data should show up in the `"AI Spans"` tab on the `"Explore" > "Traces"` page on Sentry.io.

If you manually created an <PlatformLink to="/tracing/instrumentation/custom-instrumentation/ai-agents-module/#invoke-agent-span">Invoke Agent Span</PlatformLink> (not done in the example above) the data will also show up in the [AI Agents Dashboard](/product/insights/ai/agents).

It may take a couple of moments for the data to appear in [sentry.io](https://sentry.io).

## Supported Operations

The integration automatically captures spans for the following LiteLLM calls:

- `completion()` - Chat completion requests (sync and async)
- `embedding()` - Embedding requests (sync and async)

## Options

Configure the LiteLLM integration by adding `LiteLLMIntegration` to your `sentry_sdk.init()` call:

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(
            include_prompts=True,
            # See below for all available options
        ),
    ],
)
```

### `include_prompts`

_Type: `boolean`_

Records inputs and outputs from LLM calls (prompts, messages, responses).

Defaults to `True` if `send_default_pii` is `True`.

## Supported Versions

- LiteLLM: 1.77.0+
- Python: 3.8+
