---
title: LangChain
description: "Adds instrumentation for LangChain."
---

This integration connects Sentry with [LangChain](https://github.com/langchain-ai/langchain) to automatically capture spans for AI agent interactions. Data shows up in the [AI Agents Dashboard](/product/insights/ai/agents).

## Install

Install `sentry-sdk` from PyPI:

```bash {tabTitle:pip}
pip install sentry-sdk
```

```bash {tabTitle:uv}
uv add sentry-sdk
```

## Configure

If you have the `langchain` package in your dependencies, the LangChain integration will be enabled automatically when you initialize the Sentry SDK.

```python {tabTitle:OpenAI}
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration
from sentry_sdk.integrations.openai import OpenAIIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    environment="local",
    traces_sample_rate=1.0,
    send_default_pii=True,
    debug=True,
    integrations=[
        LangchainIntegration(),
    ]
)
```

```python {tabTitle:Anthropic}
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration
from sentry_sdk.integrations.anthropic import AnthropicIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    environment="local",
    traces_sample_rate=1.0,
    send_default_pii=True,
    debug=True,
    integrations=[
        LangchainIntegration(),
    ]
)

```

## Verify

Verify that the integration works by initializing a transaction and invoking an agent. In these examples, we're providing a function tool to roll a die.

```python {tabTitle:OpenAI}
import random

from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

@tool
def roll_die(sides: int = 6) -> str:
    """Roll a die with a given number of sides"""
    return f"Rolled a {random.randint(1, sides)} on a {sides}-sided die."


with sentry_sdk.start_transaction(name="langchain-openai"):
    model = init_chat_model(
        "gpt-4o-mini",
        model_provider="openai",
        model_kwargs={"stream_options": {"include_usage": True}},
    )
    tools = [roll_die]
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="Greet the user and use the die roll tool. Do not terminate before using the tool."
            ),
            HumanMessage(content="{input}"),
            MessagesPlaceholder("agent_scratchpad"),
        ]
    )

    agent = create_openai_functions_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    result = agent_executor.invoke(
        {
            "input": "Hello, my name is Alice! Please roll a six-sided die.",
            "chat_history": [],
        }
    )
    print(result)
```

```python {tabTitle:Anthropic}
import random

from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

@tool
def roll_die(sides: int = 6) -> str:
    """Roll a die with a given number of sides"""
    return f"Rolled a {random.randint(1, sides)} on a {sides}-sided die."


with sentry_sdk.start_transaction(name="langchain-anthropic"):
    model = init_chat_model(
        "claude-3-5-sonnet-20241022",
        model_provider="anthropic",
    )
    tools = [roll_die]
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="Greet the user and use the die roll tool. Do not terminate before using the tool."
            ),
            HumanMessage(content="{input}"),
            MessagesPlaceholder("agent_scratchpad"),
        ]
    )

    agent = create_tool_calling_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    result = agent_executor.invoke(
        {
            "input": "Hello, my name is Alice! Please roll a six-sided die.",
            "chat_history": [],
        }
    )
    print(result)
```

After running this script, the resulting data should show up in the `"AI Spans"` tab on the `"Explore" > "Traces"` page on Sentry.io, and in the [AI Agents Dashboard](/product/insights/ai/agents).

It may take a couple of moments for the data to appear in [sentry.io](https://sentry.io).

## Supported Operations

The integration automatically captures spans for:

- Chat model invocations
- LLM pipeline executions
- Chain invocations
- Tool executions

## Options

Configure the LangChain integration by adding `LangchainIntegration` to your `sentry_sdk.init()` call:

```python
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    send_default_pii=True,
    integrations=[
        LangchainIntegration(
            include_prompts=True,
            # See below for all available options
        ),
    ],
)
```

### `include_prompts`

_Type: `boolean`_

Records inputs and outputs from LLM calls (prompts, messages, responses).

Defaults to `True` if `send_default_pii` is `True`.

## Supported Versions

- OpenAI: 1.0+
- Python: 3.9+
- langchain: 0.1.11+
    