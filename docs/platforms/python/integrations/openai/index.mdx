---
title: OpenAI
description: "Adds instrumentation for the OpenAI Python SDK."
---

This integration connects Sentry with the [OpenAI Python SDK](https://github.com/openai/openai-python) to automatically capture spans for LLM requests. Data shows up in the [AI Agents Dashboard](/product/insights/ai/agents).

## Install

Install `sentry-sdk` from PyPI:

```bash {tabTitle:pip}
pip install sentry-sdk
```

```bash {tabTitle:uv}
uv add sentry-sdk
```

## Configure

If you have the `openai` package in your dependencies, the OpenAI integration will be enabled automatically when you initialize the Sentry SDK.

An additional dependency, `tiktoken`, is required if you want to calculate token usage for streaming chat responses.

<PlatformContent includePath="getting-started-config" />

## Verify

Verify that the integration works by making a chat request to OpenAI.

```python
import sentry_sdk
from openai import OpenAI

sentry_sdk.init(...)  # same as above

client = OpenAI(api_key="(your OpenAI key)")

def my_llm_stuff():
    with sentry_sdk.start_transaction(
        name="The result of the AI inference",
        op="ai-inference",
    ):
      print(
          client.chat.completions.create(
              model="gpt-3.5", messages=[{"role": "system", "content": "say hello"}]
          )
          .choices[0]
          .message.content
      )
```

After running this script, the resulting data should show up in the `"AI Spans"` tab on the `"Explore" > "Traces"` page on Sentry.io.

If you manually created an <PlatformLink to="/tracing/instrumentation/custom-instrumentation/ai-agents-module/#invoke-agent-span">Invoke Agent Span</PlatformLink> (not done in the example above) the data will also show up in the [AI Agents Dashboard](/product/insights/ai/agents).

It may take a couple of moments for the data to appear in [sentry.io](https://sentry.io).

## Supported Operations

The integration automatically captures spans for the following OpenAI SDK calls:

- `chat.completions.create()` - Chat completion requests
- `responses.create()` - Response API requests
- `embeddings.create()` - Embedding requests

All exceptions leading to an `OpenAIException` are reported.

## Options

Configure the OpenAI integration by adding `OpenAIIntegration` to your `sentry_sdk.init()` call:

```python
import sentry_sdk
from sentry_sdk.integrations.openai import OpenAIIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    send_default_pii=True,
    integrations=[
        OpenAIIntegration(
            include_prompts=True,
            tiktoken_encoding_name="cl100k_base",
            # See below for all available options
        ),
    ],
)
```

### `include_prompts`

_Type: `boolean`_

Records inputs and outputs from LLM calls (prompts, messages, responses).

Defaults to `True` if `send_default_pii` is `True`.

### `tiktoken_encoding_name`

_Type: `string`_

Enables token usage calculation for streaming responses. Requires the [tiktoken](https://pypi.org/project/tiktoken/) package. See the [OpenAI Cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for possible values.

Defaults to `None`.

## Supported Versions

- OpenAI: 1.0+
- tiktoken: 0.6.0+
- Python: 3.9+
- Sentry Python SDK 2.41.0+
