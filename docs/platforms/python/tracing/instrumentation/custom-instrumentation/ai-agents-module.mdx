---
title: Instrument AI Agents
sidebar_order: 500
description: "Monitor AI agents with token usage, latency, tool execution, and error tracking."
---

With <Link to="/product/insights/ai/agents/dashboard/">Sentry AI Agent Monitoring</Link>, you can monitor and debug your AI systems with full-stack context. Track token usage, latency, tool usage, and error rates—all connected to your existing Sentry data like logs, errors, and traces.

## Prerequisites

Before setting up AI Agent Monitoring, ensure you have <PlatformLink to="/tracing/">tracing enabled</PlatformLink> in your Sentry configuration.

## Automatic Instrumentation

The Python SDK supports automatic instrumentation for AI libraries. Add the integration for your AI library to your Sentry configuration:

- <PlatformLink to="/integrations/anthropic/">Anthropic</PlatformLink>
- <PlatformLink to="/integrations/google-genai/">Google Gen AI</PlatformLink>
- <PlatformLink to="/integrations/openai/">OpenAI</PlatformLink>
- <PlatformLink to="/integrations/openai-agents/">OpenAI Agents SDK</PlatformLink>
- <PlatformLink to="/integrations/langchain/">LangChain</PlatformLink>
- <PlatformLink to="/integrations/langgraph/">LangGraph</PlatformLink>
- <PlatformLink to="/integrations/litellm/">LiteLLM</PlatformLink>
- <PlatformLink to="/integrations/pydantic-ai/">Pydantic AI</PlatformLink>

## Manual Instrumentation

If you're using a library without automatic instrumentation, you can manually instrument your code to capture spans. For your AI agents data to show up in Sentry [AI Agents Insights](https://sentry.io/orgredirect/organizations/:orgslug/insights/ai/agents/), spans must have well-defined names and data attributes.

### AI Request Span

This span represents a request to an LLM model or service that generates a response based on the input prompt.

**Key attributes:**
- `gen_ai.request.model` — The model name (required)
- `gen_ai.request.messages` — The prompts sent to the LLM
- `gen_ai.response.text` — The model's response
- `gen_ai.usage.input_tokens` / `output_tokens` — Token counts

```python
import json
import sentry_sdk

messages = [{"role": "user", "content": "Tell me a joke"}]

with sentry_sdk.start_span(op="gen_ai.request", name="request o3-mini") as span:
    span.set_data("gen_ai.request.model", "o3-mini")
    span.set_data("gen_ai.request.messages", json.dumps(messages))

    # Call your LLM here
    result = client.chat.completions.create(model="o3-mini", messages=messages)

    span.set_data("gen_ai.response.text", json.dumps([result.choices[0].message.content]))
    span.set_data("gen_ai.usage.input_tokens", result.usage.prompt_tokens)
    span.set_data("gen_ai.usage.output_tokens", result.usage.completion_tokens)
```

<Expandable title="All AI Request span attributes">
  <Include name="tracing/ai-agents-module/ai-client-span" />
</Expandable>

### Invoke Agent Span

This span represents the execution of an AI agent, capturing the full lifecycle from receiving a task to producing a final response.

**Key attributes:**
- `gen_ai.agent.name` — The agent's name (e.g., "Weather Agent")
- `gen_ai.request.model` — The underlying model used
- `gen_ai.response.text` — The agent's final output
- `gen_ai.usage.input_tokens` / `output_tokens` — Total token counts

```python
import sentry_sdk

with sentry_sdk.start_span(op="gen_ai.invoke_agent", name="invoke_agent Weather Agent") as span:
    span.set_data("gen_ai.request.model", "o3-mini")
    span.set_data("gen_ai.agent.name", "Weather Agent")

    # Run the agent
    result = my_agent.run()

    span.set_data("gen_ai.response.text", str(result.output))
    span.set_data("gen_ai.usage.input_tokens", result.usage.input_tokens)
    span.set_data("gen_ai.usage.output_tokens", result.usage.output_tokens)
```

<Expandable title="All Invoke Agent span attributes">
  <Include name="tracing/ai-agents-module/invoke-agent-span" />
</Expandable>

### Execute Tool Span

This span represents the execution of a tool or function that was requested by an AI model, including the input arguments and resulting output.

**Key attributes:**
- `gen_ai.tool.name` — The tool's name (e.g., "get_weather")
- `gen_ai.tool.input` — The arguments passed to the tool
- `gen_ai.tool.output` — The tool's return value

```python
import json
import sentry_sdk

with sentry_sdk.start_span(op="gen_ai.execute_tool", name="execute_tool get_weather") as span:
    span.set_data("gen_ai.tool.name", "get_weather")
    span.set_data("gen_ai.tool.input", json.dumps({"location": "Paris"}))

    # Call the tool
    result = get_weather(location="Paris")

    span.set_data("gen_ai.tool.output", json.dumps(result))
```

<Expandable title="All Execute Tool span attributes">
  <Include name="tracing/ai-agents-module/execute-tool-span" />
</Expandable>

### Handoff Span

This span marks the transition of control from one agent to another, typically when the current agent determines another agent is better suited to handle the task.

**Requirements:**
- `op` must be `"gen_ai.handoff"`
- `name` should follow the pattern `"handoff from {source} to {target}"`

The handoff span itself has no body—it just marks the transition point before the target agent starts.

```python
import sentry_sdk

with sentry_sdk.start_span(op="gen_ai.handoff", name="handoff from Weather Agent to Travel Agent"):
    pass  # Handoff span just marks the transition

with sentry_sdk.start_span(op="gen_ai.invoke_agent", name="invoke_agent Travel Agent"):
    # Run the target agent here
    pass
```

<Expandable title="All Handoff span attributes">
  <Include name="tracing/ai-agents-module/handoff-span" />
</Expandable>

## Common Span Attributes

<Include name="tracing/ai-agents-module/common-span-attributes" />
