---
title: Instrument AI Agents
sidebar_order: 500
description: "Learn how to manually instrument your code to use Sentry's Agents module."
---

With <Link to="/product/insights/ai/agents/dashboard/">Sentry AI Agent Monitoring</Link>, you can monitor and debug your AI systems with full-stack context. You'll be able to track key insights like token usage, latency, tool usage, and error rates. AI Agent Monitoring data will be fully connected to your other Sentry data like logs, errors, and traces.

As a prerequisite to setting up AI Agent Monitoring with Python, you'll need to first <PlatformLink to="/tracing/">set up tracing</PlatformLink>. Once this is done, the Python SDK will automatically instrument AI agents created with supported libraries. If that doesn't fit your use case, you can use custom instrumentation described below.

## Automatic Instrumentation

The Python SDK supports automatic instrumentation for some AI libraries. We recommend adding their integrations to your Sentry configuration to automatically capture spans for AI agents.

- <PlatformLink to="/integrations/anthropic/">Anthropic</PlatformLink>
- <PlatformLink to="/integrations/openai/">OpenAI</PlatformLink>
- <PlatformLink to="/integrations/openai-agents/">OpenAI Agents SDK</PlatformLink>
- <PlatformLink to="/integrations/langchain/">LangChain</PlatformLink>

## Manual Instrumentation

For your AI agents data to show up in the Sentry [AI Agents Insights](https://sentry.io/orgredirect/organizations/:orgslug/insights/agents/), two spans must be created and have well-defined names and data attributes. See below.

## Spans

### Invoke Agent Span

<Include name="tracing/ai-agents-module/invoke-agent-span" />

#### Example Invoke Agent Span:

```python
import sentry_sdk

sentry_sdk.init(...)

# some example agent implementation for demonstration
my_agent = MyAgent(
    name="Weather Agent",
    model_provider="openai",
    model="o3-mini",
)

with sentry_sdk.start_span(
    op="gen_ai.invoke_agent",
    name=f"invoke_agent {my_agent.name}",
) as span:
    # set data about LLM and agent
    span.set_data("gen_ai.operation.name", "invoke_agent")
    span.set_data("gen_ai.system", my_agent.model_provider)
    span.set_data("gen_ai.request.model", my_agent.model)
    span.set_data("gen_ai.agent.name", my_agent.name)

    # run the agent
    result = my_agent.run()

    # set agent response
    # we assume result.output is a dictionary
    # type of `gen_ai.response.text` needs to be a string
    span.set_data("gen_ai.response.text", json.dumps([result.output]))

    # set token usage
    # we assume the result includes the tokens used
    span.set_data("gen_ai.usage.input_tokens", result.usage.input_tokens)
    span.set_data("gen_ai.usage.output_tokens", result.usage.output_tokens)
```

### AI Client Span

<Include name="tracing/ai-agents-module/ai-client-span" />

#### Example AI Client Span

```python
import sentry_sdk

sentry_sdk.init(...)

# some example implementation for demonstration
my_ai = MyAI(
    model_provider="openai",
    model="o3-mini",
    model_config={
        "temperature": 0.1,
        "presence_penalty": 0.5,
    }
)

with sentry_sdk.start_span(
    op="gen_ai.chat",
    name=f"chat {my_ai.model}",
) as span:
    # set data about LLM
    span.set_data("gen_ai.operation.name", "chat")
    span.set_data("gen_ai.system", my_ai.model_provider)
    span.set_data("gen_ai.request.model", my_ai.model)

    # set up messages for LLM
    max_tokens = 1024
    prompt = "Tell me a joke"
    messages = [
        {"role": "user", "content": prompt},
    ]

    # set chat request data
    span.set_data("gen_ai.request.message", json.dumbs(messages))
    span.set_data("gen_ai.request.max_tokens", max_tokens)
    span.set_data(
        "gen_ai.request.temperature",
        my_ai.model_config.get("temperature"),
    )
    span.set_data(
        "gen_ai.request.presence_penalty",
        my_ai.model_config.get("presence_penalty"),
    )

    # ask the LLM
    result = my_ai.messages.create(
        messages=messages,
        max_tokens=max_tokens,
    )

    # set response
    # we assume result.output is a dictionary
    # type of `gen_ai.response.text` needs to be a string
    span.set_data("gen_ai.response.text", json.dumps([result.output]))

    # set token usage
    # we assume the result includes the tokens used
    span.set_data("gen_ai.usage.input_tokens", result.usage.input_tokens)
    span.set_data("gen_ai.usage.output_tokens", result.usage.output_tokens)
```

### Execute Tool Span

<Include name="tracing/ai-agents-module/execute-tool-span" />

#### Example Execute Tool Span

```python
import sentry_sdk

sentry_sdk.init(...)

# some example implementation for demonstration
my_ai = MyAI(
    model_provider="openai",
    model="o3-mini",
)

with sentry_sdk.start_span(op="gen_ai.chat", ...):
    #  .. some code ..
    result = my_ai.messages.create(
        messages=messages,
        max_tokens=1024,
    )
    # .. some code ..

# we assume the llm tells us to call a tool in the result
if my_should_call_tool(result):
    # we parse the result to know which tool to call
    tool = my_get_tool_to_call(result)

    with sentry_sdk.start_span(
        op="gen_ai.execute_tool",
        name=f"execute_tool {tool.name}"
    ) as span:
        # set data about LLM and tool
        span.set_data("gen_ai.system", my_ai.model_provider)
        span.set_data("gen_ai.request.model", my_ai.model)
        span.set_data("gen_ai.tool.type", "function")
        span.set_data("gen_ai.tool.name", tool.name)
        span.set_data("gen_ai.tool.description", tool.description)

        # run tool
        tool_result = my_run_tool(tool)

        # set tool result
        span.set_data("gen_ai.tool.output", json.dumps(tool_result))
```

### Handoff Span

<Include name="tracing/ai-agents-module/handoff-span" />

#### Example Handoff Span

```python
import sentry_sdk

sentry_sdk.init(...)

# some example agent implementation for demonstration
my_agent = MyAgent(
    name="Weather Agent",
    model_provider="openai",
    model="o3-mini",
)

with sentry_sdk.start_span(op="gen_ai.invoke_agent", ...):
    #  .. some code ..
    result = my_agent.run()
    #  .. some code ..


# we assume the llm tells us to handoff to another agent
if my_should_handoff(result):
    # we parse the result to know which agent to handoff to
    other_agent = my_get_agent(result)

    with sentry_sdk.start_span(
        op="gen_ai.handoff",
        name=f"handoff from {my_agent.name} to {other_agent.name}",
    ):
        # the handoff span just marks the handoff
        pass

    with sentry_sdk.start_span(op="gen_ai.invoke_agent", ...):
        #  .. some code ..
        other_agent.run()
        #  .. some code ..
```

## Common Span Attributes

<Include name="tracing/ai-agents-module/common-span-attributes" />
