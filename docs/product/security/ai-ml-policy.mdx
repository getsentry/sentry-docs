---
title: "AI/ML: Transparency and Choice"
sidebar_order: 10
description: "Learn about Sentry's approach to AI/ML"
---

Throughout Sentry’s history, we’ve operated under a policy of [privacy by default](https://sentry.io/lp/privacy-by-default/). This same principal applies to our work in the Artificial Intelligence (AI) and Machine Learning (ML) space, where we want to be just as [transparent](https://blog.sentry.io/terms-of-service-update/) about what data we’re using and why.

Sentry is at a juncture where prior heuristics-based approaches cannot sustain the demands of the product. For example, fingerprinting error events as part of creating groups, has gotten a lot more complicated with the rise of JavaScript and the use of extensions and third-party services.

To train and validate models for grouping, notifications, and workflow improvements, Sentry will need access to additional service data to deliver a better user experience.

You can update these settings within the new “Consent to Use Service Data” section of the **Legal & Compliance** page in [Sentry](https://sentry.sentry.io/orgredirect/organizations/:orgslug/settings/legal/), which is located within the “Usage & Billing” Settings.

## Use of Non-Identifying Data

In accordance with our Terms of Service, Sentry may use non-identifying elements of your service data for product improvement. For example, we may aggregate web vitals data to show your site’s performance against a Sentry-built benchmark. The data accessed for the benchmark cannot be linked back to any particular project or customer, making it non-identifying.

## Use of Aggregated Identifying Data

For upcoming features like priority alerts or ML-based grouping, Sentry is asking for access to the following forms of service data:

- Error messages
- Stack traces
- Spans
- DOM interactions

Our intended improvements will harness text embeddings – converting words into numerical values that capture their semantic information. In this context, we’ll treat embeddings as a set of features for a downstream task. For example, we may use this aggregated dataset to train a model that will be able to predict the severity of a new issue. **The information within these embeddings can't identify specific organizations, projects, or issues**. While model inputs will use customer data, the outputs will never risk exposing customer information.

## Use of Identifying Data for Generative AI Features

For upcoming features like Autofix that use Generative AI and Retrieval Augmented Generation (RAG), Sentry is asking for access to the following forms of service data:

- Stack traces
- Relevant code to linked repositories

For those use cases, we'll use text embeddings as an index - a way to retrieve the most contextually relevant bits of source code (via our GitHub integration) or events for a particular Sentry issue. When it comes to RAG, embeddings will always be logically separated (never crossing customer boundaries).

All functionality leveraging RAG will require user opt-in. By opting in, you agree to send relevant stack trace and code from linked repositories to third-party machine learning subprocessors, as disclosed in our [subprocessor list](https://sentry.io/legal/subprocessors/). If you don't want access to these new features, nothing will change.

## Data Access Summary

<table>
  <tr>
    <th>Access Type</th>
    <th>Is the underlying data identifiable?</th>
    <th>Will this data (or any output) be shared with others?</th>
    <th>Will this data be used for training Sentry models?</th>
    <th>Will this data be used to train 3rd party models?</th>
  </tr>
  <tr>
    <th>Non-identifying data</th>
    <td>No</td>
    <td>Other Sentry customers</td>
    <td>Yes</td>
    <td>No</td>
  </tr>
  <tr>
    <th>Aggregated identifying data</th>
    <td>Yes</td>
    <td>Sentry only</td>
    <td>Yes</td>
    <td>No</td>
  </tr>
  <tr>
    <th>Identifying data for generative AI features</th>
    <td>Yes</td>
    <td>Approved AI subprocessors</td>
    <td>No</td>
    <td>No</td>
  </tr>
</table>

## Data Handling

In addition to the consent mechanisms mentioned above:

1. We'll continue to encourage all customers to use our [various data scrubbing tools](https://docs.sentry.io/product/data-management-settings/scrubbing/) so that service data is sanitized before we receive it.
2. We'll apply the same deletion and retention rules to our training data as we do to the underlying service data. This means that if you delete service data, it will also be removed from our machine learning models automatically.
3. We'll scrub data for PII before it goes into any training set.
4. We'll ensure that the only service data presented in the output of any ML feature belongs to the customer using the feature.
5. We'll only use AI models built in-house or provided by our existing trusted [third-party sub-processors](https://sentry.io/legal/subprocessors/) who have made contractual commitments that are consistent with the above.

We're confident that with these controls in place, we'll be able to use service data to improve our products through AI while at the same time protecting that data.
