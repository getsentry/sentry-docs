---
title: The Dashboard
sidebar_order: 100
---


Once you've [configured the Sentry SDK](/product/ai-monitoring/getting-started/) for your AI project, you'll start
receiving data in the Sentry AI Monitoring dashboard.

![AI Monitoring Dashboard](../img/pipelines-view.png)


## The per-pipeline dashboard
In this case, there are two Langchain pipelines (whose `.name` is the name that shows up in the table).
One has used 58,000 tokens in the past hour, and the other has used 44,900 tokens.

When you click one of the pipelines in the table, you can see details about that particular pipeline.

![AI Monitoring for a specific pipeline](../img/details-view.png)

The "Ask Sentry" pipeline has used 59 thousand tokens, and taken 3.2 seconds on average.

<Note>
An AI pipeline is different than simply calling an LLM. If you are creating AI pipelines by calling LLMs directly (without using a tool like Langchain),
    consider using [Manual AI Instrumentation](/product/ai-monitoring/getting-started/#manually-instrumenting-ai-workloads)
</Note>

## Where AI Data Shows Up In the Trace View

If configured to include PII, the Sentry SDK will add prompts and responses to LLMs and other AI models to spans in the trace view.

![AI Monitoring trace example](../img/trace-view.png)

In the example above, you can see input messages and LLM responses related to the `ai.chat_completions.create.langchain` span. Other spans like `ai.chat_completions.create.openai` show the number of tokens used for that particular chat completion.

This view can show other data as well. For example, if you call your LLM from a webserver, the trace will include details about the webserver through other integrations, and you'll get a holistic view of all the related parts.
