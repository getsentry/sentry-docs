---
title: The Dashboard
sidebar_order: 100
---


Once you've [configured the Sentry SDK](/product/ai-monitoring/getting-started/) for your AI project, you'll start
receiving data in the Sentry AI Monitoring dashboard.

![AI Monitoring Dashboard](../img/pipelines-view.png)


## The per-pipeline dashboard
In this case, there are two Langchain pipelines (whose `.name` is the name that shows up in the table).
One has used 58,000 tokens in the past hour, and the other has used 44,900 tokens.

When you click one of the pipelines in the table, you can see details about that particular pipeline.

![AI Monitoring for a specific pipeline](../img/details-view.png)

The "Ask Sentry" pipeline has used 59 thousand tokens, and took 3.2 seconds on average.

Additionally, we can see example traces which include this pipeline in the table at the bottom.

## Where AI data shows up in the trace view

![AI Monitoring trace example](../img/trace-view.png)

The Sentry SDK will (if configured to include PII) add the prompts and responses to LLMs and other AI models to
the spans in the Trace view.

In the picture above, you can see that the `ai.chat_completions.create.langchain` span includes the input messages.
Other spans like `ai.chat_completions.create.openai` include the number of tokens used for that particular
chat completion.

You can use the existing Sentry Performance features to identify which parts of your AI pipeline are taking a long time,
and which ones generated bad LLM output or used a lot of tokens.
