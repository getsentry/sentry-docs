---
title: Set Up
sidebar_order: 0
description: "Learn how to set up Sentry AI Monitoring"
---

Sentry AI Monitoring is easiest to use with the Python SDK and one of our official integrations.

![AI Monitoring User Interface](../img/pipelines-view.png)


To get started, make sure you've created a sentry project for your AI-enabled repository, and follow one of the guides
below to start sending AI data to Sentry.

## Official AI integrations

- [OpenAI](/platforms/python/integrations/openai/)
- [Langchain](/platforms/python/integrations/langchain/)

<Alert level="note" title="Don't see your platform?">

We will be adding AI integrations continuously. You can also instrument AI manually with the Sentry Python SDK.

</Alert>


## Manually instrumenting AI workloads

If you aren't using an orchestrator like Langchain or a known LLM provider like OpenAI, you can still use
Sentry AI monitoring in two ways:

### Python SDK Decorators

The [Python SDK](/platforms/python) includes an `@ai_track` decorator which will mark functions as AI-related and
make them show up in the AI Monitoring dashboard.

```python

import time
from sentry_sdk.ai_analytics import ai_track, record_token_usage
import sentry_sdk
import requests

@ai_track(description="AI tool")
def some_workload_function():
    """
    This function is an example of calling arbitrary code with @ai_track so that it shows up in the Sentry trace
    """
    time.sleep(5)

@ai_track(description="LLM")
def some_llm_call():
    """
    This function is an example of calling an LLM provider that isn't officially supported by Sentry.
    """
    with sentry_sdk.start_span(op="ai.chat_completions.create.examplecom", description="Example.com LLM") as span:
        result = requests.get('https://example.com/api/llm-chat?question=say+hello').json()
        # this annotates the tokens used by the LLM so that they show up in the graphs in the dashboard
        record_token_usage(span, total_tokens=result["usage"]["total_tokens"])
        return result["text"]

@ai_track(description="My AI pipeline")
def some_pipeline():
    """
    The topmost level function with @ai_track gets the operation "ai.pipeline", which makes it show up
    in the table of AI pipelines in the Sentry AI Monitoring dashboard.
    """
    client = OpenAI()
    some_workload_function()
    some_llm_call()
    response = (
        client.chat.completions.create(
            model="some-model", messages=[{"role": "system", "content": "say hello"}]
        )
        .choices[0]
        .message.content
    )
    print(response)

with sentry_sdk.start_transaction(op="ai-inference", name="The result of the AI inference"):
    some_pipeline()

```

