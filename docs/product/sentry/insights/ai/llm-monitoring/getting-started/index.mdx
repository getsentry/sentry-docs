---
title: Set Up
sidebar_order: 0
description: "Learn how to set up Sentry LLM Monitoring"
---

Sentry LLM Monitoring helps you track and debug AI-powered applications using our supported SDKs and integrations.

![LLM Monitoring User Interface](../img/pipelines-view.png)

To start sending LLM data to Sentry, make sure you've created a Sentry project for your AI-enabled repository and follow one of the guides below:

## Supported SDKs

### Python

The Sentry Python SDK supports LLM monitoring with integrations for OpenAI, Langchain, Anthropic, Huggingface, and Cohere

#### Official AI Integrations

- <LinkWithPlatformIcon
    platform="openai"
    label="OpenAI"
    url="/platforms/python/integrations/openai/"
  />
- <LinkWithPlatformIcon
    platform="langchain"
    label="Langchain"
    url="/platforms/python/integrations/langchain/"
  />
- <LinkWithPlatformIcon
    platform="anthropic"
    label="Anthropic"
    url="/platforms/python/integrations/anthropic/"
  />
- <LinkWithPlatformIcon
    platform="huggingface"
    label="Huggingface Hub"
    url="/platforms/python/integrations/huggingface_hub/"
  />
- <LinkWithPlatformIcon
    platform="python"
    label="Cohere"
    url="/platforms/python/integrations/cohere/"
  />

### JavaScript

The JavaScript SDK supports LLM monitoring through the Vercel AI integration for Node.js and Bun runtimes.

#### Supported Platforms

- <LinkWithPlatformIcon
    platform="javascript.node"
    label="Node.js"
    url="/platforms/javascript/guides/node/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.nextjs"
    label="Next.js"
    url="/platforms/javascript/guides/nextjs/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.sveltekit"
    label="SvelteKit"
    url="/platforms/javascript/guides/sveltekit/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.nuxt"
    label="Nuxt"
    url="/platforms/javascript/guides/nuxt/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.astro"
    label="Astro"
    url="/platforms/javascript/guides/astro/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.remix"
    label="Remix"
    url="/platforms/javascript/guides/remix/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.solidstart"
    label="SolidStart"
    url="/platforms/javascript/guides/solidstart/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.express"
    label="Express"
    url="/platforms/javascript/guides/express/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.fastify"
    label="Fastify"
    url="/platforms/javascript/guides/fastify/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.nestjs"
    label="Nest.js"
    url="/platforms/javascript/guides/nestjs/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.hapi"
    label="Hapi"
    url="/platforms/javascript/guides/hapi/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.koa"
    label="Koa"
    url="/platforms/javascript/guides/koa/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.connect"
    label="Connect"
    url="/platforms/javascript/guides/connect/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.hono"
    label="Hono"
    url="/platforms/javascript/guides/hono/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.bun"
    label="Bun"
    url="/platforms/javascript/guides/bun/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.aws-lambda"
    label="AWS Lambda"
    url="/platforms/javascript/guides/aws-lambda/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.azure-functions"
    label="Azure Functions"
    url="/platforms/javascript/guides/azure-functions/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.gcp-functions"
    label="Google Cloud Functions"
    url="/platforms/javascript/guides/gcp-functions/configuration/integrations/vercelai/"
  />
- <LinkWithPlatformIcon
    platform="javascript.electron"
    label="Electron"
    url="/platforms/javascript/guides/electron/configuration/integrations/vercelai/"
  />

<Alert title="Don't see your platform?">

We'll be adding AI integrations continuously. You can also instrument AI manually with the Sentry Python SDK.

</Alert>

## Pipelines and LLMs

The Sentry LLM Monitoring feature relies on the fact that you have an orchestrator (like LangChain) creating pipelines of one or more LLMs (such as gpt-4). In the LLM Monitoring dashboard, we show you a table of the AI pipelines and pull the token usage from your LLMs.

If you're using a provider like OpenAI without an orchestrator like LangChain, you'll need to manually create pipelines with the `@ai_track` annotation. If you're using a non-supported LLM provider and want to record token usage, use the `record_token_usage()` helper function. Both manual helpers are documented below.

### Python SDK Decorators

The [Python SDK](/platforms/python) includes an `@ai_track` decorator which will mark functions as AI-related and
cause them to show up in the LLM Monitoring dashboard.

```python

import time
from sentry_sdk.ai.monitoring import ai_track, record_token_usage
import sentry_sdk
import requests

@ai_track("AI tool")
def some_workload_function(**kwargs):
    """
    This function is an example of calling arbitrary code with @ai_track so that it shows up in the Sentry trace
    """
    time.sleep(5)

@ai_track("LLM")
def some_llm_call():
    """
    This function is an example of calling an LLM provider that isn't officially supported by Sentry.
    """
    with sentry_sdk.start_span(op="ai.chat_completions.create.examplecom", name="Example.com LLM") as span:
        result = requests.get('https://example.com/api/llm-chat?question=say+hello').json()
        # this annotates the tokens used by the LLM so that they show up in the graphs in the dashboard
        record_token_usage(span, total_tokens=result["usage"]["total_tokens"])
        return result["text"]

@ai_track("My AI pipeline")
def some_pipeline():
    """
    The topmost level function with @ai_track gets the operation "ai.pipeline", which makes it show up
    in the table of AI pipelines in the Sentry LLM Monitoring dashboard.
    """
    client = OpenAI()
    # Data can be passed to the @ai_track annotation to include metadata
    some_workload_function(sentry_tags={"username": "my_user"}, sentry_data={"data": "some longer data that provides context"})
    some_llm_call()
    response = (
        client.chat.completions.create(
            model="some-model", messages=[{"role": "system", "content": "say hello"}]
        )
        .choices[0]
        .message.content
    )
    print(response)

with sentry_sdk.start_transaction(op="ai-inference", name="The result of the AI inference"):
    some_pipeline()

```
