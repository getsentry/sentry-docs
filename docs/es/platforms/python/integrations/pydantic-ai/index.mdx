---
title: Pydantic AI
description: "Aprende a usar Sentry con Pydantic AI."
---

<Alert title="Beta">

El soporte para **Pydantic AI** está en beta. Prueba primero de forma local antes de usarlo en producción.

</Alert>

Esta integración conecta Sentry con la biblioteca [Pydantic AI](https://ai.pydantic.dev/).
Se ha confirmado que la integración funciona con Pydantic AI a partir de la versión 1.0.0.

Una vez que instales esta integración, podrás usar [Sentry AI Agents Insights](https://sentry.io/orgredirect/organizations/:orgslug/insights/ai/agents/), un panel de Sentry que te ayuda a entender qué está pasando con tus agentes de IA.

La supervisión de Sentry AI Agents recopilará automáticamente información sobre agentes, herramientas, prompts, tokens y modelos.

<div id="install">
  ## Instalación
</div>

Instala `sentry-sdk` desde PyPI:

```bash {tabTitle:pip}
pip install "sentry-sdk"
```

```bash {tabTitle:uv}
uv add "sentry-sdk"
```


<div id="configure">
  ## Configuración
</div>

Añade `PydanticAIIntegration()` a tu lista de `integrations`:

```python {tabTitle:OpenAI}
import sentry_sdk
from sentry_sdk.integrations.pydantic_ai import PydanticAIIntegration
from sentry_sdk.integrations.openai import OpenAIIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    traces_sample_rate=1.0,
    # Añade datos como entradas/salidas de LLM y herramientas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
    send_default_pii=True,
    integrations=[
        PydanticAIIntegration(),
    ],
    # Desactiva la integración de OpenAI para evitar el doble reporte de spans de chat
    disabled_integrations=[OpenAIIntegration()],
)
```

```python {tabTitle:Anthropic}
import sentry_sdk
from sentry_sdk.integrations.pydantic_ai import PydanticAIIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    traces_sample_rate=1.0,
    # Agrega datos como entradas/salidas de LLM y herramientas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más info
    send_default_pii=True,
    integrations=[
        PydanticAIIntegration(),
    ],
)
```

<Alert level="warning">
  Al usar Pydantic AI con modelos de OpenAI, debes desactivar la integración de OpenAI para evitar el doble registro de spans de chat. Agrega `disabled_integrations=[OpenAIIntegration()]` a tu llamada a `sentry_sdk.init()`, como se muestra en la pestaña de OpenAI de arriba.
</Alert>


<div id="verify">
  ## Verificar
</div>

Verifica que la integración funcione ejecutando un agente de IA. Los datos resultantes deberían aparecer en tu panel de Insights de AI Agents. En este ejemplo, creamos un agente de soporte al cliente que analiza las consultas de los clientes y, de forma opcional, puede buscar información de pedidos usando una herramienta.

```python {tabTitle:OpenAI}
import asyncio

import sentry_sdk
from sentry_sdk.integrations.pydantic_ai import PydanticAIIntegration
from sentry_sdk.integrations.openai import OpenAIIntegration
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel

class SupportResponse(BaseModel):
    message: str
    sentiment: str
    requires_escalation: bool

support_agent = Agent(
    'openai:gpt-4o-mini',
    name="Agente de Atención al Cliente",
    system_prompt=(
        "Eres un agente de atención al cliente servicial. Analiza las consultas de los clientes, "
        "proporciona respuestas útiles y determina si es necesario escalar el caso. "
        "Si el cliente menciona un número de pedido, usa la herramienta de búsqueda para obtener los detalles."
    ),
    result_type=SupportResponse,
)

@support_agent.tool
async def lookup_order(ctx: RunContext[None], order_id: str) -> dict:
    """Busca los detalles del pedido por ID de pedido.
    
    Args:
        ctx: El objeto de contexto.
        order_id: El identificador del pedido.
    
    Returns:
        Detalles del pedido incluyendo estado y seguimiento.
    """
    # En una aplicación real, esto consultaría una base de datos
    return {
        "order_id": order_id,
        "status": "enviado",
        "tracking_number": "1Z999AA10123456784",
        "estimated_delivery": "2024-03-15"
    }

async def main() -> None:
    sentry_sdk.init(
        dsn="___PUBLIC_DSN___",
        traces_sample_rate=1.0,
        # Agrega datos como entradas/salidas de LLM y herramientas;
        # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
        send_default_pii=True,
        integrations=[
            PydanticAIIntegration(),
        ],
        # Deshabilita la integración de OpenAI para evitar el doble reporte de spans de chat
        disabled_integrations=[OpenAIIntegration()],
    )

    result = await support_agent.run(
        "Hola, tengo una consulta sobre mi pedido #ORD-12345. ¿Cuándo llegará?"
    )
    print(result.data)

if __name__ == "__main__":
    asyncio.run(main())
```

```python {tabTitle:Anthropic}
import asyncio

import sentry_sdk
from sentry_sdk.integrations.pydantic_ai import PydanticAIIntegration
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel

class SupportResponse(BaseModel):
    message: str
    sentiment: str
    requires_escalation: bool

support_agent = Agent(
    'anthropic:claude-3-5-sonnet-latest',
    name="Agente de Atención al Cliente",
    system_prompt=(
        "Eres un agente de atención al cliente servicial. Analiza las consultas de los clientes, "
        "proporciona respuestas útiles y determina si es necesario escalar el caso. "
        "Si el cliente menciona un número de pedido, usa la herramienta de búsqueda para obtener los detalles."
    ),
    result_type=SupportResponse,
)

@support_agent.tool
async def lookup_order(ctx: RunContext[None], order_id: str) -> dict:
    """Busca los detalles del pedido por ID de pedido.
    
    Args:
        ctx: El objeto de contexto.
        order_id: El identificador del pedido.
    
    Returns:
        Detalles del pedido incluyendo estado y seguimiento.
    """
    # En una aplicación real, esto consultaría una base de datos
    return {
        "order_id": order_id,
        "status": "enviado",
        "tracking_number": "1Z999AA10123456784",
        "estimated_delivery": "2024-03-15"
    }

async def main() -> None:
    sentry_sdk.init(
        dsn="___PUBLIC_DSN___",
        traces_sample_rate=1.0,
        # Agrega datos como entradas/salidas de LLM y herramientas;
        # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
        send_default_pii=True,
        integrations=[
            PydanticAIIntegration(),
        ],
    )

    result = await support_agent.run(
        "Hola, tengo una duda sobre mi pedido #ORD-12345. ¿Cuándo llegará?"
    )
    print(result.data)

if __name__ == "__main__":
    asyncio.run(main())
```

Puede que los datos tarden unos momentos en aparecer en [sentry.io](https://sentry.io).


<div id="behavior">
  ## Comportamiento
</div>

Se recopilarán datos sobre lo siguiente:

- invocaciones de agentes de IA
- ejecución de herramientas
- número de tokens de entrada y de salida utilizados
- uso de modelos de LLM
- configuraciones del modelo (temperature, max_tokens, etc.)

Sentry considera las entradas y salidas de LLM y de las herramientas como PII y no incluye datos de PII de forma predeterminada. Si quieres incluirlos, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente los prompts y las salidas a pesar de `send_default_pii=True`, configura la integración con `include_prompts=False` como se muestra en la sección de Opciones a continuación.

<div id="options">
  ## Opciones
</div>

Si añades `PydanticAIIntegration` explícitamente en tu llamada a `sentry_sdk.init()`, puedes establecer opciones para `PydanticAIIntegration` y ajustar su comportamiento:

```python
import sentry_sdk
from sentry_sdk.integrations.pydantic_ai import PydanticAIIntegration

sentry_sdk.init(
    # ...
    # Añade datos como entradas y respuestas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
    send_default_pii=True,
    integrations=[
        PydanticAIIntegration(
            include_prompts=False,  # Las entradas/salidas de LLM y herramientas no se enviarán a Sentry, a pesar de send_default_pii=True
        ),
    ],
)
```

Puedes pasar los siguientes argumentos con nombre a `PydanticAIIntegration()`:

* `include_prompts`:

  Indica si las entradas y salidas del LLM y de las herramientas deben enviarse a Sentry. De forma predeterminada, Sentry considera estos datos como información de identificación personal (PII). Si deseas incluirlos, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente los prompts y los resultados a pesar de `send_default_pii=True`, configura la integración con `include_prompts=False`.

  El valor predeterminado es `True`.


<div id="supported-versions">
  ## Versiones compatibles
</div>

- Pydantic AI: 1.0.0+
- Python: 3.9+