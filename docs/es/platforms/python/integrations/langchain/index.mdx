---
title: LangChain
description: "Aprende a usar Sentry con LangChain."
---

Esta integración conecta Sentry con [LangChain](https://github.com/langchain-ai/langchain) en Python.

Una vez que instales este SDK, podrás usar Sentry AI Agents Monitoring, un panel de Sentry que te ayuda a entender qué ocurre con tus solicitudes de IA. Sentry AI Monitoring recopilará automáticamente información sobre prompts, herramientas, tokens y modelos. Obtén más información sobre el [AI Agents Dashboard](/es/product/insights/ai/agents).

<div id="install">
  ## Instalación
</div>

Instala `sentry-sdk` desde PyPI con el extra `langchain`:

```bash {tabTitle:pip}
pip install "sentry-sdk[langchain]"
```

```bash {tabTitle:uv}
uv add "sentry-sdk[langchain]"
```

<div id="configure">
  ## Configurar
</div>

Si tienes el paquete `langchain` en tus dependencias, la integración de LangChain se habilitará automáticamente al inicializar el SDK de Sentry. Para contabilizar correctamente los tokens, debes desactivar la integración del proveedor de modelos que estés usando (p. ej., OpenAI o Anthropic).

```python {tabTitle:OpenAI}
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration
from sentry_sdk.integrations.openai import OpenAIIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    environment="local",
    traces_sample_rate=1.0,
    send_default_pii=True,
    debug=True,
    integrations=[
        LangchainIntegration(),
    ],
    disabled_integrations=[OpenAIIntegration()],
)
```

```python {tabTitle:Anthropic}
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration
from sentry_sdk.integrations.anthropic import AnthropicIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    environment="local",
    traces_sample_rate=1.0,
    send_default_pii=True,
    debug=True,
    integrations=[
        LangchainIntegration(),
    ],
    disabled_integrations=[AnthropicIntegration()],
)

```

<div id="verify">
  ## Verificar
</div>

Comprueba que la integración funciona iniciando una transacción e invocando un agente. En estos ejemplos, proporcionamos una herramienta de función para lanzar un dado.

```python {tabTitle:OpenAI}
import random

from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

@tool
def roll_die(sides: int = 6) -> str:
    """Lanza un dado con un número dado de caras"""
    return f"Salió un {random.randint(1, sides)} en un dado de {sides} caras."


with sentry_sdk.start_transaction(name="langchain-openai"):
    model = init_chat_model(
        "gpt-4o-mini",
        model_provider="openai",
        model_kwargs={"stream_options": {"include_usage": True}},
    )
    tools = [roll_die]
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="Saluda al usuario y usa la herramienta de lanzamiento de dado. No termines antes de usar la herramienta."
            ),
            HumanMessage(content="{input}"),
            MessagesPlaceholder("agent_scratchpad"),
        ]
    )

    agent = create_openai_functions_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    result = agent_executor.invoke(
        {
            "input": "¡Hola, mi nombre es Alice! Por favor, lanza un dado de seis caras.",
            "chat_history": [],
        }
    )
    print(result)
```

```python {tabTitle:Anthropic}
import random

from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

@tool
def roll_die(sides: int = 6) -> str:
    """Lanza un dado con un número dado de caras"""
    return f"Salió un {random.randint(1, sides)} en un dado de {sides} caras."


with sentry_sdk.start_transaction(name="langchain-anthropic"):
    model = init_chat_model(
        "claude-3-5-sonnet-20241022",
        model_provider="anthropic",
    )
    tools = [roll_die]
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="Saluda al usuario y usa la herramienta de lanzar dado. No termines antes de usar la herramienta."
            ),
            HumanMessage(content="{input}"),
            MessagesPlaceholder("agent_scratchpad"),
        ]
    )

    agent = create_tool_calling_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    result = agent_executor.invoke(
        {
            "input": "¡Hola, me llamo Alice! Por favor lanza un dado de seis caras.",
            "chat_history": [],
        }
    )
    print(result)
```

Después de ejecutar este script, los datos resultantes deberían aparecer en la pestaña &quot;AI Spans&quot; de la página &quot;Explore&quot; &gt; &quot;Traces&quot; en Sentry.io, y en el [AI Agents Dashboard](/es/product/insights/ai/agents).

Es posible que los datos tarden unos momentos en aparecer en [sentry.io](https://sentry.io).

<div id="behavior">
  ## Comportamiento
</div>

* La integración de LangChain conectará Sentry automáticamente con todos los métodos de LangChain compatibles.

* Se registran todas las excepciones.

* Sentry considera las entradas y salidas de LLM y del tokenizador como PII (información de identificación personal) y no incluye PII de forma predeterminada. Si quieres incluirla, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente las indicaciones (prompts) y salidas a pesar de `send_default_pii=True`, configura la integración con `include_prompts=False`, como se indica en la [sección Opciones](#options) a continuación.

<div id="options">
  ## Opciones
</div>

Al agregar explícitamente `LangchainIntegration` a tu llamada a `sentry_sdk.init()`, puedes configurar opciones para `LangchainIntegration` y cambiar su comportamiento:

```python
import sentry_sdk
from sentry_sdk.integrations.langchain import LangchainIntegration

sentry_sdk.init(
    # ...
    # Añade datos como entradas y respuestas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para obtener más información
    send_default_pii=True,
    integrations=[
        LangchainIntegration(
            include_prompts=False,  # Las entradas y salidas del LLM no se enviarán a Sentry, a pesar de send_default_pii=True
        ),
    ],
)
```

Puedes pasar los siguientes argumentos con nombre a `LangchainIntegration()`:

* `include_prompts`

  Indica si las entradas y salidas del LLM y del tokenizador deben enviarse a Sentry. Sentry considera estos datos como información de identificación personal (PII) de forma predeterminada. Si quieres incluir los datos, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente los prompts y las salidas aun con `send_default_pii=True`, configura la integración con `include_prompts=False`.

  El valor predeterminado es `True`.

<div id="supported-versions">
  ## Versiones compatibles
</div>

* OpenAI: 1.0+
* Python: 3.9+
* LangChain: 0.1.11+