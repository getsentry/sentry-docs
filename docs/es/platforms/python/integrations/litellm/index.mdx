---
title: LiteLLM
description: "Obtén información sobre cómo usar Sentry con LiteLLM."
---

Esta integración conecta Sentry con el [SDK de Python de LiteLLM](https://github.com/BerriAI/litellm).

Una vez que instales este SDK, podrás usar Sentry AI Agents Monitoring, un panel de Sentry que te ayuda a entender qué sucede con tus solicitudes de IA.

Sentry AI Monitoring recopilará automáticamente información sobre prompts, herramientas, tokens y modelos. Obtén más información sobre el [panel de agentes de IA](/es/product/insights/ai/agents).

<div id="install">
  ## Instalar
</div>

Instala `sentry-sdk` desde PyPI con el extra `litellm`:

```bash {tabTitle:pip}
pip install "sentry-sdk[litellm]"
```

```bash {tabTitle:uv}
uv add "sentry-sdk[litellm]"
```

<div id="configure">
  ## Configuración
</div>

Añade `LiteLLMIntegration()` a tu lista de `integrations`:

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    # Configura traces_sample_rate en 1.0 para capturar el 100 %
    # de las transacciones en el trazado.
    traces_sample_rate=1.0,
    # Incluye datos como entradas y respuestas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(),
    ],
)
```

<div id="verify">
  ## Verificar
</div>

Comprueba que la integración funciona haciendo una solicitud de finalización de chat a LiteLLM.

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration
import litellm

sentry_sdk.init(
    dsn="___PUBLIC_DSN___",
    traces_sample_rate=1.0,
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(),
    ],
)

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "di hola"}],
    max_tokens=100
)
print(response.choices[0].message.content)
```

Después de ejecutar este script, los datos resultantes deberían aparecer en la pestaña &quot;AI Spans&quot; de la página &quot;Explore&quot; &gt; &quot;Traces&quot; en Sentry.io.

Si creaste manualmente un <PlatformLink to="/tracing/instrumentation/custom-instrumentation/ai-agents-module/#invoke-agent-span">Invoke Agent Span</PlatformLink> (algo que no se hace en el ejemplo anterior), los datos también aparecerán en el [AI Agents Dashboard](/es/product/insights/ai/agents).

Es posible que los datos tarden unos momentos en aparecer en [sentry.io](https://sentry.io).

<div id="behavior">
  ## Comportamiento
</div>

* La integración de LiteLLM conectará Sentry con los métodos compatibles de LiteLLM automáticamente.

* Las funciones compatibles actualmente son `completion` y `embedding` (tanto síncronas como asíncronas).

* Sentry considera las entradas y salidas de los LLM como PII (información de identificación personal) y no incluye estos datos de forma predeterminada. Si quieres incluirlos, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente los prompts y las salidas aunque `send_default_pii=True` esté activado, configura la integración con `include_prompts=False`, como se indica en la [sección de opciones](#options) a continuación.

<div id="options">
  ## Opciones
</div>

Si agregas explícitamente `LiteLLMIntegration` a tu llamada a `sentry_sdk.init()`, puedes establecer opciones para `LiteLLMIntegration` y modificar su comportamiento:

```python
import sentry_sdk
from sentry_sdk.integrations.litellm import LiteLLMIntegration

sentry_sdk.init(
    # ...
    # Añade datos como entradas y respuestas;
    # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para obtener más información
    send_default_pii=True,
    integrations=[
        LiteLLMIntegration(
            include_prompts=False,  # Las entradas/salidas del LLM no se enviarán a Sentry, a pesar de send_default_pii=True
        ),
    ],
)
```

Puedes pasar los siguientes argumentos con nombre a `LiteLLMIntegration()`:

* `include_prompts`:

  Indica si las entradas y salidas del LLM deben enviarse a Sentry. Sentry considera estos datos como información de identificación personal (PII) de forma predeterminada. Si quieres incluirlos, establece `send_default_pii=True` en la llamada a `sentry_sdk.init()`. Para excluir explícitamente los prompts y las salidas incluso con `send_default_pii=True`, configura la integración con `include_prompts=False`.

  El valor predeterminado es `True`.

<div id="supported-versions">
  ## Versiones compatibles
</div>

* LiteLLM: 1.77.0+
* Python: 3.8+