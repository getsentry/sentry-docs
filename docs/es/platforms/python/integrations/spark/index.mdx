---
title: Apache Spark
description: "Aprende a usar Sentry con Apache Spark."
---

La integración de Spark añade compatibilidad con la API de Python para Apache Spark, [PySpark](https://spark.apache.org/).

**Esta integración es experimental y está en fase alfa**. La API de la integración puede sufrir cambios incompatibles en futuras versiones menores.

<div id="driver">
  ### Controlador
</div>

La integración del controlador de Spark es compatible con Spark 2 y versiones posteriores.

Para configurar el SDK, inicialízalo con la integración antes de crear un `SparkContext` o `SparkSession`.

Además de capturar errores, puedes supervisar las interacciones entre varios servicios o aplicaciones [habilitando el tracing](/es/concepts/key-terms/tracing/). También puedes recopilar y analizar perfiles de rendimiento de usuarios reales con [profiling](/es/product/explore/profiling/).

Selecciona qué funciones de Sentry quieres instalar, además del monitoreo de errores, para obtener a continuación las instrucciones correspondientes de instalación y configuración.

<OnboardingOptionButtons
  options={[
'error-monitoring',
'performance',
'profiling',
]}
/>

```python
import sentry_sdk
from sentry_sdk.integrations.spark import SparkIntegration

if __name__ == "__main__":
    sentry_sdk.init(
        dsn="___PUBLIC_DSN___",
        # Agrega datos como encabezados de solicitud e IP de usuarios, si corresponde;
        # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
        send_default_pii=True,
        # ___PRODUCT_OPTION_START___ performance
        # Establece traces_sample_rate en 1.0 para capturar el 100%
        # de las transacciones para el rastreo.
        traces_sample_rate=1.0,
        # ___PRODUCT_OPTION_END___ performance
        # ___PRODUCT_OPTION_START___ profiling
        # Para recopilar perfiles de todas las sesiones de perfilado,
        # establece `profile_session_sample_rate` en 1.0.
        profile_session_sample_rate=1.0,
        # Los perfiles se recopilarán automáticamente mientras
        # haya un span activo.
        profile_lifecycle="trace",
        # ___PRODUCT_OPTION_END___ profiling
        integrations=[
            SparkIntegration(),
        ],
    )

    spark = SparkSession\
        .builder\
        .appName("ExampleApp")\
        .getOrCreate()
    ...
```

<div id="worker">
  ### Worker
</div>

La integración del worker de Spark es compatible con Spark 2.4.x y 3.1.x.

Crea un archivo llamado `sentry-daemon.py` con el siguiente contenido:

```python {filename:sentry-daemon.py}
import sentry_sdk
from sentry_sdk.integrations.spark import SparkWorkerIntegration
import pyspark.daemon as original_daemon

if __name__ == '__main__':
    sentry_sdk.init(
        dsn="___PUBLIC_DSN___",
        # Agrega datos como encabezados de solicitud e IP de usuarios, si aplica;
        # consulta https://docs.sentry.io/platforms/python/data-management/data-collected/ para más información
        send_default_pii=True,
        # ___PRODUCT_OPTION_START___ performance
        # Establece traces_sample_rate en 1.0 para capturar el 100%
        # de las transacciones para el rastreo.
        traces_sample_rate=1.0,
        # ___PRODUCT_OPTION_END___ performance
        # ___PRODUCT_OPTION_START___ profiling
        # Para recopilar perfiles de todas las sesiones de perfilado,
        # establece `profile_session_sample_rate` en 1.0.
        profile_session_sample_rate=1.0,
        # Los perfiles se recopilarán automáticamente mientras
        # haya un span activo.
        profile_lifecycle="trace",
        # ___PRODUCT_OPTION_END___ profiling
        integrations=[
            SparkWorkerIntegration(),
        ],
    )

    original_daemon.manager()
    ...
```

En tu comando `spark_submit`, añade las siguientes opciones de configuración para que los clústeres de Spark puedan usar la integración con Sentry.

| Opciones de línea de comandos | Parámetro                                | Uso                                                          |
| ----------------------------- | ---------------------------------------- | ------------------------------------------------------------ |
| --py-files                    | sentry&#95;daemon.py                     | Envía el archivo `sentry_daemon.py` a tus clústeres de Spark |
| --conf                        | spark.python.use.daemon=true             | Configura Spark para usar un daemon para ejecutar sus workers de Python |
| --conf                        | spark.python.daemon.module=sentry&#95;daemon | Configura Spark para usar el daemon personalizado de Sentry  |

```bash
./bin/spark-submit \
    --py-files sentry_daemon.py \
    --conf spark.python.use.daemon=true \
    --conf spark.python.daemon.module=sentry_daemon \
    example-spark-job.py
```

<div id="behavior">
  ## Comportamiento
</div>

* Debes tener el SDK de Sentry para Python instalado en todos tus clústeres para usar la integración con Spark. La manera más sencilla de hacerlo es ejecutar un script de inicialización en todos tus clústeres:

```bash
easy_install pip
pip install --upgrade sentry-sdk
```

* Para acceder a ciertas etiquetas (`app_name`, `application_id`), la integración del worker requiere que la integración del driver también esté activa.

* La integración del worker solo funciona en sistemas basados en UNIX porque el proceso daemon utiliza señales para gestionar procesos hijo.

<div id="google-cloud-dataproc">
  ## Google Cloud Dataproc
</div>

Esta integración se puede configurar para [Google Cloud Dataproc](https://cloud.google.com/dataproc/). Se recomienda usar la versión de imagen de Cloud Dataproc 1.4 o 2.0 con Spark 2.4 y 3.1, respectivamente (según lo requiera la integración del worker).

1. Configura una [Initialization action](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions) para instalar `sentry-sdk` en tu clúster de Dataproc.

2. Agrega la integración del driver a tu archivo principal de Python, enviado en la pantalla de envío del job.

3. Agrega `sentry_daemon.py` en <i>Additional python files</i> en la pantalla de envío del job. Primero debes subir el archivo del daemon a un bucket para poder acceder a él.

4. Agrega, en la pantalla de envío del job, las propiedades de configuración indicadas arriba: `spark.python.use.daemon=true` y `spark.python.daemon.module=sentry_daemon`.