---
title: LangChain
description: "Añade instrumentación para LangChain."
supported:
  - javascript.node
  - javascript.aws-lambda
  - javascript.azure-functions
  - javascript.connect
  - javascript.express
  - javascript.fastify
  - javascript.gcp-functions
  - javascript.hapi
  - javascript.hono
  - javascript.koa
  - javascript.nestjs
  - javascript.electron
  - javascript.nextjs
  - javascript.nuxt
  - javascript.solidstart
  - javascript.sveltekit
  - javascript.react-router
  - javascript.remix
  - javascript.astro
  - javascript.bun
  - javascript.tanstackstart-react
  - javascript.cloudflare
---

<Alert>

Esta integración funciona en los runtimes de Node.js, Cloudflare Workers y Vercel Edge Functions. Requiere la versión `10.22.0` o superior del SDK.

</Alert>

_Nombre de importación: `Sentry.langChainIntegration`_

La `langChainIntegration` añade instrumentación para LangChain a fin de capturar spans envolviendo automáticamente las operaciones de LangChain y registrando las interacciones del agente de IA, con grabación configurable de entradas y salidas.

<PlatformSection notSupported={["javascript.cloudflare", "javascript.nextjs"]}>

Está habilitada de forma predeterminada y capturará automáticamente spans de operaciones de LangChain, incluidas los modelos de chat, invocaciones de LLM, cadenas (chains) y ejecuciones de herramientas. Puedes activar la captura de entradas y salidas estableciendo `recordInputs` y `recordOutputs` en la configuración de la integración:

```javascript
Sentry.init({
  dsn: "____PUBLIC_DSN____",
  tracesSampleRate: 1.0,
  integrations: [
    Sentry.langChainIntegration({
      recordInputs: true,
      recordOutputs: true,
    }),
  ],
});
```

</PlatformSection>

<PlatformSection supported={["javascript.cloudflare"]}>

Para Cloudflare Workers, debes instrumentar manualmente las operaciones de LangChain usando el helper `createLangChainCallbackHandler`:

```javascript
import * as Sentry from "@sentry/cloudflare";
import { ChatAnthropic } from "@langchain/anthropic";

// Crea un manejador de callbacks de LangChain
const callbackHandler = Sentry.createLangChainCallbackHandler({
  recordInputs: true, // Opcional: registrar prompts/mensajes de entrada
  recordOutputs: true, // Opcional: registrar respuestas de salida
});

// Úsalo con modelos de chat
const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-20241022",
  apiKey: process.env.ANTHROPIC_API_KEY,
});

await model.invoke("Tell me a joke", {
  callbacks: [callbackHandler],
});
```

</PlatformSection>

<div id="options">
  ## Opciones
</div>

<div id="recordinputs">
  ### `recordInputs`
</div>

*Tipo: `boolean`*

Registra las entradas en operaciones de LangChain (como indicaciones y mensajes).

De forma predeterminada, es `true` si `sendDefaultPii` es `true`.

```javascript
Sentry.init({
  integrations: [Sentry.langChainIntegration({ recordInputs: true })],
});
```


<div id="recordoutputs">
  ### `recordOutputs`
</div>

*Tipo: `boolean`*

Registra los resultados de las operaciones de LangChain (como texto generado y respuestas).

De forma predeterminada es `true` si `sendDefaultPii` es `true`.

```javascript
Sentry.init({
  integrations: [Sentry.langChainIntegration({ recordOutputs: true })],
});
```


<div id="configuration">
  ## Configuración
</div>

De forma predeterminada, esta integración añade compatibilidad de trazas para las operaciones de LangChain, incluidas:

- **Invocaciones de modelos de chat** (`gen_ai.chat`) - Captura spans de las llamadas a modelos de chat
- **Invocaciones de LLM** (`gen_ai.pipeline`) - Captura spans de las ejecuciones del pipeline de LLM
- **Ejecuciones de cadenas** (`gen_ai.invoke_agent`) - Captura spans de las invocaciones de cadenas
- **Ejecuciones de herramientas** (`gen_ai.execute_tool`) - Captura spans de las llamadas a herramientas

<div id="supported-runnables">
  ### Runnables compatibles
</div>

La integración instrumenta automáticamente los siguientes métodos de runnable de LangChain:

- `invoke()` - Ejecución única
- `stream()` - Ejecución en streaming
- `batch()` - Ejecución por lotes

<PlatformSection notSupported={["javascript.cloudflare", "javascript.nextjs"]}>

<div id="supported-providers">
  ### Proveedores compatibles
</div>

La instrumentación automática es compatible con los siguientes paquetes de proveedores de LangChain:

- `@langchain/anthropic`
- `@langchain/openai`
- `@langchain/google-genai`
- `@langchain/mistralai`
- `@langchain/google-vertexai`
- `@langchain/groq`

</PlatformSection>

<PlatformSection supported={['javascript.nextjs']}>

<div id="edge-runtime">
  ## Edge runtime
</div>

Esta integración se instrumenta automáticamente en el runtime de Node.js. Para aplicaciones de Next.js que usan el Edge runtime, debes instrumentar manualmente las operaciones de LangChain usando el callback handler:

```javascript
import * as Sentry from "@sentry/nextjs";
import { ChatAnthropic } from "@langchain/anthropic";

// Crea un callback handler de LangChain
const callbackHandler = Sentry.createLangChainCallbackHandler({
  recordInputs: true,
  recordOutputs: true,
});

// Úsalo con modelos de chat
const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-20241022",
  apiKey: process.env.ANTHROPIC_API_KEY,
});

await model.invoke("Tell me a joke", {
  callbacks: [callbackHandler],
});
```

</PlatformSection>

<div id="supported-versions">
  ## Versiones compatibles
</div>

- `langchain`: `>=0.1.0 <1.0.0`